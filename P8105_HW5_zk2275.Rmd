---
title: "P8105: Data Science I"
author: "Hw5<br>Zhuodiao Kuang(UNI: zk2275)"
output:
  github_document:
    toc: TRUE
---

<!------------------------------------------------------------------------------------------
Preamble
------------------------------------------------------------------------------------------->

```{r, packages loading and default set,echo = FALSE, message = FALSE, warning = FALSE}
# load necessary packages
library(tidyverse)
library(dplyr)
library(readxl)
library(rvest)
library(kableExtra)

# set knitr defaults
knitr::opts_chunk$set(
               echo      = TRUE,
	             cache     = TRUE,
               prompt    = FALSE,
               tidy      = FALSE,
               comment   = NA,
               message   = FALSE,
               warning   = FALSE,
               dpi       = 150,
               fig.align = "center")
# set theme defaults
theme_set(
  theme_bw() +
  theme(
    legend.position = "bottom"
    , plot.title    = element_text(hjust = 0.5)
    , plot.subtitle = element_text(hjust = 0.5)    
    , plot.caption  = element_text(hjust = 0.0)
  )
)

# set color scale defaults
options(
    ggplot2.continuous.colour = "gradient"
  , ggplot2.continuous.fill   = "gradient"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete   = scale_fill_viridis_d
```

<!------------------------------------------------------------------------------------------
Problem 1
------------------------------------------------------------------------------------------->

# Problem 1

Import the data set

```{r homicide_read}
path <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide_df = 
  read_csv(path, na = c("", "Unknown"))|>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) |>
  # remove a row that seems to have an error
  filter(city_state != "Tulsa, AL")
  
head(homicide_df, 10) |> knitr::kable()
```

Upon filtering out a single row with a data entry issue, the dataset consists of `r nrow(homicide_df)` observations and 14 variables, two of which - city_state and resolution - we added for convenience. Each row of the dataset corresponds to a homicide case reported between 2007 and 2017 and includes record of the victim’s name, race/ethnicity, age, and sex.

We also have data on the location of the crime down to the latitude and longitude, as well as whether the crime was ever solved. Across all observations, about 50.8% of all homicides were never solved. 

We aim to understand the distribution of resolution rates across cities. To that end, we provide a table of the number of unsolved homicides and the total number of hommicides for each city.

```{r}
aggregate_df = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Do a prop test for Baltimore, MD.

```{r}
# Save the output
Baltimore_prop <- prop.test(
  aggregate_df |> filter(city_state == "Baltimore, MD") |> pull(hom_unsolved), 
  aggregate_df |> filter(city_state == "Baltimore, MD") |> pull(hom_total)) 

# pull out
broom::tidy(Baltimore_prop) |> 
  # extract relevant output
  select(estimate, conf.low, conf.high)

```

We find that in Baltimore between the years 2007 and 2017, the proportion of homicides that went unsolved was about 64.6%, and the 95% confidence interval around this point estimate was about [62.8%, 66.3%]. As we have obtained the desired output, we are now prepared to construct a function that can generalize this procedure.

```{r}
results_df = 
  aggregate_df |>
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) |> 
  # extract both the proportion of unsolved homicides and the confidence interval for each
  select(-prop_tests) |> 
  unnest(tidy_tests) |> 
  select(city_state, estimate, conf.low, conf.high)

results_df |> knitr::kable()
```

Using this data frame of results, we can now better visualize the distribution of proportions, sorted according to the point estimate of the proportion of unsolved homicides.

```{r homicide plot}
# create error bar chart of rate of unsolved homicide by city
results_df |>
  # reorder cities according to point estimate of proportion
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  # instantiate plot
  ggplot(aes(x = city_state, y = estimate)) +
  # add point estimates
  geom_point() +
  # add confidence intervals
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  # flip axes for readability
  coord_flip() +
  # add meta-data
  labs(
    title = "Proportion of Unsolved Homicides, by City"
    , x = "Proportion"
    , y = ""
    , caption  = paste0(
          "Note: Confidence intervals computed at the 95% cinfidence level."
        , "\nSource: The Washington Post."
    )
  )
```

Richmond, VA has the lowest rate of unsolved homicides among these 50 cities. Chicago, IL sits at the other extreme of this distribution. In fact, as its confidence interval does not overlap with that of any other city, it would appear to be an outlier in this sense.


<!------------------------------------------------------------------------------------------
Problem 2
------------------------------------------------------------------------------------------->

# Problem 2

We next consider (ostensibly fictional) data from a longitudinal study that included a control arm and an experimental arm.

# create tidy dataset for longitudinal study
```{r}
lda_df = 
  tibble(
    file = list.files("datasets/hw5_data/data"),
  ) %>% 
  mutate(
    path = str_c("datasets/hw5_data/data/", file),
    # read in data now
    data = map(path, read_csv)
    ) %>% 
  unnest(data) %>% 
  # extract the group and id
  mutate(
    label = str_extract(file, "(exp_[0-9][0-9]|con_[0-9][0-9])"),
  ) %>% 
  separate(
    label,
    into = c("arm","id"),
    sep = "_"
  ) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observations",
    names_prefix = "week_"
  ) %>% 
  mutate(
    arm = as.factor(arm),
    id = as.numeric(id),
    week = as.numeric(week),
    observations = as.numeric(observations)
  ) %>% 
  select(-file,-path)

lda_df %>%
  head(10) %>% 
  knitr::kable()
```

Each subject’s weekly values were originally stored as a row vector in an isolated CSV file. We have tidied the data by iterating over each file and building a singular dataset wherein each observation in the dataset is uniquely identified by the arm type (control or experimental), the subject ID, and the week. It is now a simple task to visualize the trend of values over time by arm type.

# create a spaghetti chart of the data
```{r}
lda_df %>%
  # instantiate plot
  ggplot(aes(x = week, y = observations, group = id)) +
  # add lines
  geom_line() +
  # create separate line charts for each arm
  facet_grid(~arm) +
  # add meta-data
  labs(
      title = "Value over Time, by Arm"
    , x     = "Week"
    , y     = "Value"
  )
```


We see that values for the control arm group trended flat, if not slightly downward, over the course of the eight weeks of the trial. While values for the experimental arm group generally started in the same place as those of the control arm group, the average subject in the experimental arm group exhibited a linear increase of almost 4 units over the course of the trial. While not a formal statistical hypothesis test, this visualization gives us hope that the experimental drug was effective.

<!------------------------------------------------------------------------------------------
Problem 3
------------------------------------------------------------------------------------------->

# Problem 3

